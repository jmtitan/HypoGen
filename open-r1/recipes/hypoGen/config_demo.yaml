# File parsed by TrlParser -> model_args, training_args, script_args
# Model arguments
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
#dataset_name: open-r1/OpenR1-Math-220k
dataset_name: Dudep/retweet_all
system_prompt: '''
**Instruction**  
You are a social media expert. Your task has two parts:

### Part 1: Hypothesis Generation
Given a set of tweet observations, you will generate hypotheses that are useful for predicting which tweet out of a pair will be retweeted more.
- Each tweet pair is posted by the same user and contains similar content with slight wording differences.
- Focus on these wording differences.
- Please generate 1 hypotheses in the format:
  HP: [hypothesis]
- Make your hypotheses general enough to apply to new tweet pairs.

### Part 2: Hypothesis-Based Inference
Using the hypotheses you just generated, apply them to a given pair of tweets.
- Predict which tweet will be retweeted more based on the learned patterns.
- Answer in the format:
  **Final answer: the _ tweet** (where `_` is either `first` or `second`)

Think step by step:
1. Can your hypothesis apply to the tweets?
2. Analyze the textual differences.
3. Decide which tweet is more likely to be retweeted.
4. Provide your final prediction.

'''
user_prompt: '''Generate 1 hypotheses that are useful for predicting which tweet in a pair is more likely to be retweeted.

Then, use these hypotheses to decide which tweet will get more retweets.

**Learned hypotheses:**
_(To be generated above)_

**Tweet Pair:**
- First tweet: {first_tweet}
- Second tweet: {second_tweet}

Please follow the reasoning steps and give the final answer.
**Final answer: the _ tweet**
'''
# GRPO trainer confsig
bf16: true
use_vllm: true
vllm_device: auto
vllm_gpu_memory_utilization: 0.7
do_eval: false
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: Qwen2.5-1.5B-Open-R1-GRPO
hub_strategy: every_save
learning_rate: 2.0e-05
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_prompt_length: 512
max_completion_length: 1024
max_steps: -1
num_generations: 4
num_train_epochs: 1
output_dir: data/Qwen2.5-1.5B-hypogen-GRPO
overwrite_output_dir: true
per_device_eval_batch_size: 4
per_device_train_batch_size: 4
push_to_hub: false
report_to:
- wandb
reward_funcs:
- eqn_eval 
- format
- tag_count
reward_weights:
- 1.0
- 1.0
- 1.0
save_strategy: "epoch"
save_total_limit: 1
seed: 42
warmup_ratio: 0.1